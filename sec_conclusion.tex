% !TEX root = main.tex

\section{Conclusion and Discussion}\label{sec:conclusion}
% You generally cover three things in the Conclusions section, and each of these usually merits a separate subsection:

% 1. Conclusions
% 2. Summary of Contributions
% 3. Future Research

% Conclusions are not a rambling summary of the thesis: they are short, concise statements of the inferences that you have made because of your work. It helps to organize these as short numbered paragraphs, ordered from most to least important. All conclusions should be directly related to the research question stated in Section 4. Examples:

% 1. The problem stated in Section 4 has been solved: as shown in Sections ? to ??, an algorithm capable of handling large-scale Zylon problems in reasonable time has been developed.
% 2. The principal mechanism needed in the improved Zylon algorithm is the Grooty mechanism.
% 3. Etc.

% The Summary of Contributions will be much sought and carefully read by the examiners. Here you list the contributions of new knowledge that your thesis makes. Of course, the thesis itself must substantiate any claims made here. There is often some overlap with the Conclusions, but that's okay. Concise numbered paragraphs are again best. Organize from most to least important. Examples:

% 1. Developed a much quicker algorithm for large-scale Zylon problems.
% 2. Demonstrated the first use of the Grooty mechanism for Zylon calculations.
% 3. Etc.

% The Future Research subsection is included so that researchers picking up this work in future have the benefit of the ideas that you generated while you were working on the project. Again, concise numbered paragraphs are usually best. 


\subsection{Discussion}

%\henrique{What you wrote here, in breaking subsubsections for each RQ is not really a conclusion, it looks more like a "Discussion" subsection. You can put Discussion as the last part of the evaluation or in the conclusions.}

\subsubsection{RQ\#1: Can we apply SA ranking techniques in an industrial environment with limited amount of data?}

Although a dataset of around 190k collected total alerts (of which around 80k are actionable), seems largely adequate at first sight, it should be noted that more than 93\% of them belong to one of the four alert categories, \textit{cppcoreguidelines}, for the total number of alerts, as well as the actionable ones (\cref{data_analysis:collected_alerts}). Those are mostly stylistic checks, which are abundant in nature and easy to disappear with code changes. The remaining three categories contain a total of around 12k alerts, of which 3.5k are closed.

Nevertheless, the scores for each individual category are also good (\cref{results:individual_categories}), so the dominating one does not inflate the results for the rest.

Given the overall results of the best performing method, Actionable Alerts (\cref{method:actalerts}), we can say that applying SA ranking techniques is possible in this industrial context, even considering the fact that SA tools are not used anymore.

\subsubsection{RQ\#2: Can we combine SA ranking techniques to achieve better results?}

The performance of the Actionable Alerts method (\cref{results:actionable_alerts}) is difficult to improve. What we can improve by combining multiple methods, is not only ranking alerts that are actionable, but also by directing the attention to those that point to bug-prone methods (can have a higher utility by preventing bugs). In that aspect, we do achieve better results in the top 200 places in the ranking (\cref{results:ensemble_approach}).


\subsubsection{RQ\#3: Do pre-processing techniques provide a significant performance benefit?}

By inspecting the metrics achieved by different balancing/cleaning techniques (\cref{result:actalerts} and \cref{result:bugPrediction}) we can see that they do make a small difference. Although, in the Actionable Alerts method, the improvement is negligible, in the Method Bug Prediction approach, the improvement is more noticeable.

\subsection{Conclusions}
%\textbf{TO DO}\\
%-got these results, because...\\
%-importance on preprocessing\\
%-initial results may be slightly better that normal, but the nature of data collection is limiting, need to be used in continuance\\ 
\henrique{For a conclusion, you start by doing quick summarization of what you did (here you can describe everything you did that you think is important. Be sure to cite numbers as results as you describe them.}
\henrique{It is a good start for a conclusion. We usually do not reference other sections as often as you did in this conclusion, but it is not a problem. My main concern is that you should put some actual results numbers here as well. I know they are better described in the refering sections, but anyone looking at the conclusion wants to see a quick wrap up.}

We started by collecting two different datasets, one related to SA alerts and their change with time, the other about buggy methods and the changes within them (\cref{data_collection:overview}). By using those datasets we evaluated four different techniques, two which are directly concerned with finding actionable alerts (\cref{method:fbrank}, \cref{method:actalerts}), which the other two mainly focus on the bug prediction aspect (\cref{method:brls}, \cref{method:bugprediction}).
On top of that, an ensemble approach that combines both types of methods was evaluated.

Excluding the Feedback Rank method, which did not scale to the size of our test project (could be because of implementation problems), the other methods all provide a noticeable improvement over random ranking of alerts (\cref{evaluation:results}), which ranges from 50\% to 75\% more actionable alerts in the top 100 places, or from 39\% to 57\% more actionable alerts on the top 1000 places. 

The Actionable Alerts approach, provides great results in detecting actionable alerts (\cref{result:actalerts}). With a precision and recall of around 97\%, and around 50\% more actionable alerts on both the top 100 and 1000 places (compared to random ranking). 
%\henrique{wth a precision of XX\%,recall of XX\%, and...}

The Method Bug Prediction, even though does not reach a high performance in its core task (predicting buggy methods), with a precision/recall of respectively 71\% and 72\%, is nevertheless able to rank favorably actionable alerts on buggy methods (\cref{results:ranking_methodbug_alerts}). It achieves around 75\% and 57\% improvement over random ranking on the top 100 and 1000 places respectively (tested only on those alerts that are located on the methods of the test set).

The Bug-related Lines approach, despite its simplicity, still performs well (\cref{results:ranking_brls}), providing an improvement of 50\% and 39\% over random ranking on the top 100 and 1000 places respectively.
 
%\henrique{Again try to put some actual metric results here.}

By combining the best performing method for detecting actionable alerts, with the one predicting buggy methods, and simple alert weights (calculated from the bug-related alerts), we built an ensemble approach that does not only rank actionable alerts higher, but also promotes alerts that point to buggy methods higher in the ranking (\cref{results:ensemble_ranking}). By sacrificing some precision on the actionable alerts, we can promote 36\% more bug-related alerts in the top 200 places of the ranking.


\subsection{Summary of Contributions}
 
This thesis provides the following contributions:
\begin{itemize}
%\item Building a workflow to extract information by making use of the Clang toolset and version history (SVN). 
%\henrique{A contribution is something that was finished, that it is done. Anything starting with 'building' (or any other 'ing') is not a contribution because you are still doing it (as it is not finished). Even if I rewrite the sentence, I am not convinced that the workflow is a contribution.}
%\kleidi{The thesis is made in the context of an internship, where in my opinion the focus is higher on implementation than a normal research. Since a considerable amount of time was put into extraction of the data (more difficult in a restricted company environment than a typical research), it is also meant as a justification that an adequate amount of time was spent in the making of this thesis. I do agree that in the research point of view it is not something considered valuable. I do not mind if it is removed, but don't want to give the wrong impression that the thesis might be something hacked together in little time.}
%\henrique{One contribution will not be the sole cause that gives the impression of 'thesis hacked together in little time'. It is overacting to say so. I understand that you may be stressed out due to the deadline, but try to keep a cool head. Moreover, it does not matter if you chose to focus on implementation or research, a contribution in the thesis will be the same nonetheless. On that note, not everything you worked on is a contribution. I do not deny that you put a lot of work on extracting data. Now saying that is a contribution is a completely different thing. Workflows (most often) are not contributions. The process or method on a workflow could be a contribution, but I do not think yours is. The extracted data could be a contribution if it was a public dataset useful for other researchers to conduct comparative experiments, and it is not the case for your data either. Therefore, I do not see anything here that can be classified as a contribution. In the end, this is YOUR thesis, if you want to leave this as a contribution go ahead. You are more than free to follow my suggestions or not.}

\item The evaluation of different SA ranking techniques on an industrial codebase, namely: Actionable Alerts (ranking alerts depending on their historical usefulness), Method Bug Prediction (ranking alerts based on how bug-prone is the method where they are located), Bug-Related Lines (ranking alerts based on type weights calculated on bug-related and actionable alerts) and an Ensemble method (ranking alerts based on a combination of the three aforementioned methods).
%\henrique{I think you are missing a little more detail to make this into a proper contribution.}
%\kleidi{For example? I have results for each tool.}
%\henrique{For example, which ranking technique were used?}
\item Comparison of pre-processing methods to deal with imbalanced and noisy data.
\item We conducted a comparative study of different approaches on a common codebase.
%\item We demonstrate the utility of combining different ranking methods into an ensemble. \henrique{by using a Bayesian network?}
%\kleidi{this is unfortunately a to do for the moment, hope to finish it tonight/tomorrow}
%\henrique{I was wondering if the ensemble was performed by using a Bayesian network. If it is then it should be detailed in the contribution. }
%\kleidi{that is what is holding me back for the moment, how to precisely combine them}

\end{itemize}

% -building workflow to extract information\\
% -evaluation on industrial code\\
% -comparison of utility between tools\\
% -evaluation of an ensemble technique



\subsection{Future Research}

%Analyze the parts where the prediction fails?

Future research can be focused on different aspects, the most important being reliable data collection. A classifier is as good as the data it was trained on, so new ways to collect actionable alerts in a more precise way are crucial to achieving better performance. Information Retrieval or Natural Language Processing techniques can be applied for example on bug descriptions to have a clearer connection between a bug and an alert.

Given also the limited amount of (potentially) noisy data, the cleaning aspect and be explored even further (fine tuning techniques). The impact of pre-processing should also be considered, for example how to best deal with high-cardinality data (which is the case many features in out dataset).

Furthermore, there is a lot of room for fine tuning the optimization problem in the ensemble method. Its definition, the score function, the optimization objectives, can be analyzed even further. Also, the performance of Method Bug Prediction, can be a bottleneck for this approach, so other methods which may give better results can be tried (\cite{predict_deeplearning, dl_jit_prediction}).

Last, a way to continuously improve the ranking algorithms needs to be put in place. If newly inspected alerts are tracked consistently (TP or FP), a more qualitative dataset can be collected and performance will also improve (a rather naive implementation is to give each warnings an identifier and include them in the commit messages if they were useful).

% -data collection assumptions, make it more reliable?\\
% -better track resolved alerts (google tricorder example)
% -information retrieval on bug messages
% TO DO
