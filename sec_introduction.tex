% !TEX root = main.tex

\section{Introduction}\label{sec:introduction}
% This is a general introduction to what the thesis is all about -- it is not just a description of the contents of each section. Briefly summarize the question (you will be stating the question in detail later), some of the reasons why it is a worthwhile question, and perhaps give an overview of your main results. This is a birds-eye view of the answers to the main questions answered in the thesis (see above). 

% +Positive aspects of SA and usages (google, fb etc...), their need (huge amounts of code), etc...\\

\henrique{Is the SA used for maintenance, development, or both? If it is maintenance, we should shift the initial paragraph wih a little motivation on maintenance. If it for both, we should focus on development but also mention maintenance activities.}
\kleidi{I know that OMP wants to use it in a CI environment, so in development}
\henrique{I would argue that CI is both development and maintenance.}
\kleidi{Basically when devs push a change, th CI will analyze it and if there are warnings it will ask the devs to pay attention to them (its very vague, and i don't have much info but i think its more development)}

%The amount of code that is being produced is increasing with time. \henrique{It is important, to put a reference when writing such strong statement. Although, I think there are better ways to start the intro.} That brings multiple challenges in the software engineering landscape, one of which is assuring adequate code quality. In this context, automatic approaches such as Static Analysis (SA) can be really useful and time saving in detecting and preventing potential bugs. \henrique{The last two sentences are confusing, first you say it is important to assure code quality. And next you talk about bugs. Those are different concepts, you can have a code with bad quality and low bugs and vice-versa. From what I understand, your goal is more related to bug prevention. Quality metrics for source cde are usually based on coupling and cohesion, and you do not measure that.} 
%Its usefulness has been acknowledged also by big tech companies, such as Google with its Tricorder architecture~\cite{sa_google} or Facebook with its Infer static analyzer~\cite{infer}.
%Another reason to adopt SA techniques, is that the cost of repairing a defect is much lower when that defect is found early in the development cycle (generally accepted principle). \henrique{Even if it is a generally acepted principle, as academic researchers we should provide a reference to it. Here it is a journal paper that shows a figure on the costs measured by the IBM System Science Institute~\cite{dawson2010}.}
%
%\henrique{Bellow is my atempt to rewrite the first paragraph. You can use whichever you prefer, or mix and mash what you want. I think there are 3 different topics above that do not match all that well. Therefore, I split it into 3 paragraphs}

Software development and maintenance is a complex activity~\cite{clarke2016}. Because of this complexity, there are many challenges in the software engineering landscape, when developing software. In this environment, automated tools and techniques matter to help the development scale with consistency~\cite{winters2020}. 

Static Analysis (SA) is an automated technique that can be useful for the development process. For example, it is possible to use SA to detect potential bugs in the source code. It is a generally accepted principle that resolving issues in earlier stages of the development cycle is less costly~\cite{dawson2010}. Therefore, the bugs detected by SA may reduce the maintenance and development costs.

Popular companies acknowledge the usefulness of Static Analysis techniques by adopting them in their process. For instance, Google with its Tricorder architecture~\cite{sa_google} or Facebook with its Infer static analyzer~\cite{infer}.

%\henrique{** End of the first paragraph rewrite suggestion **}


Even though SA is useful and employed by real companies, there is still much room for improvement. Since the software under analysis is not executed, SA tools must infer what the actual program behavior will be. Therefore, SA tools are bound to make misclassifications or raise false alarms.

Given the tendency of SA tools to over-estimate possible faulty program behaviours, there is a need to fine-tune and improve the alarms and warnings raised by these tools. There are two ways that we can improve SA tools: (i) increase the precision of the analysis which usually decreases its recall and the overall number of raised alarms; or (ii) by post-processing the alarms after they are generated, according to specific criteria more appropriate for the codebase. We opted to focus on the second option, post-processing of alarms. 
%%
The classic approach most tools use for prioritizing and filtering results is to classify the results based on severity levels. This approach is simple but oblivious to the actual analyzed code or the location and frequency of a given issue. Furthermore, it has been shown that if developers lose trust in the tool, they then tend to ignore the output of it altogether \cite{sa_google}.

Different approaches have been proposed to improve the output of SA tools. Optimally, the initial warnings report should be those most likely to be real errors. Alerts can be strategically prioritized for examination, by tracking and analyzing them through a series of software versions, we can automatically determine which SA rules are more important and which parts of the software are more problematic. 
% Furthermore, alerts can be grouped/clustered in basis of their similarity, so that users can check only a few and then can easily determine if the rest of the errors are also worth inspecting.\\
Moreover, an understanding of how developers react to these alerts can help improve the usability of these tools. Alerts can be divided into two categories: (i) actionable alerts (AA) which the programmer would act on to resolve; and (ii) unactionable alerts (UA) which the programmer would not act on~\cite{comparative_heckman, actionable_sa}. An unactionable alert may be of trivial concern to fix, less likely to manifest at runtime, or incorrectly identified due to the limitations of the tool. Thus, we want to prioritize the actionable alerts and hide from developers the unactionable ones.

Another approach is to prioritize alerts that, in the past, lead to the discovery of bugs~\cite{which_warnings,automatic_training_set}. By tracking back the bugs up to a past version, we can collect code lines that changed during bug fixes. Subsequently, we can pinpoint which alerts warned about those specific parts of code and prioritize accordingly.

For a relatively large project, the number of SA alerts can be prohibitive, which is one of the reasons developers avoid these tools~\cite{why_dont_use}. The ultimate goal is not to analyze all alerts, but to maximize the time-cost spent on them. Ranking schemes do not reduce alert investigation burdens if the aim is to check them all. Instead, they solve the problem by showing alerts that are most likely to be useful, so that developers can spend time by inspecting the most important ones.

Different approaches have been proposed in the literature but few have tried to do a comparison in terms of the utility of each method. Among those few, the comparison is mainly done between open-source Java software where a good amount of data can be extracted. In our case, we assess the utility of different approaches inside an industrial C++ codebase, where static analysis has been abandoned due to poor performance. Also, it is interesting to explore if these methods can be combined to achieve better results. Different techniques can be better suited to different types of alerts or can compensate for each others weaknesses if combined.

% (depending also on the information that can be extracted from the source code).

% -what is this student's research question?
% -is it a good question? (has it been answered before? is it a useful question to work on?) 

%Given an industrial code base, where SA tools have been abandoned because of the large amount of false positives, the goal of this thesis is to test the feasibility of successfully applying these ranking approaches even in a context with limited amount of (or noisy) data.

%\henrique{This my re-write on the previous paragraph. Again, use whichever you prefer or mix then both.}
In this thesis, our goal is to verify if it is possible to apply ranking approaches to static analysis alerts in an industrial code base with a limited amount of noisy data. We like to highlight that SA tools were initially employed and later abandoned by this company due to a large number of false positives. We claim that a ranking mechanism fined tuned to this company's code base can achieve good results and be useful to developers.

% The research questions can be formulated as follows:
% \begin{itemize}
%     \item R0: Can we apply SA ranking techniques in an industrial environment with limited amount of data (abandoned because of high false positives)?
%     \item R1: Given a highly imbalances dataset, what are the best performing dataset balancing techniques?
%     \item R2: Can we process the collected data to remove faulty examples (CLNI?) (project specific bugs?) (fix the data not the algorithms)
%     \item R3: Can we combine SA ranking techniques to achieve better results?
% \end{itemize}

% In this thesis, a comparison of different SA processing approaches is performed in an industrial codebase. Also, the utility of combining different methods is explored.

The rest of this thesis will be structured in the following sections... (\textbf{TODO})

% \textbf{TO DO: duality, actionable alert and bug predicition alerts}


% Arguments/information gotten from papers that can be used for the thesis introduction.
% \begin{itemize}
%     \item Why the false positives?
%     \begin{itemize}
%         \item Inevitable mistakes (SA has to make a trade-off with efficiency).
%         \item Static analyzers commonly use heuristics or approximations to determine properties for a given code construct, which frequently induces them to make inaccurate assumptions regarding the behavior of the program under evaluation.
%         \item A tool may deliberately introduce approximations for scalability or speed or to check richer properties than is generally possible, potentially resulting in an outbreak of errors. By ranking the results the invalid errors can be relegated below true errors.
%         \item Because the software under analysis is not executed, static analysis tools must speculate on what the actual program behavior will be. They often over-estimate possible program behaviors, leading to spurious warnings (“false positives”) that do not correspond to true defects.
%     \end{itemize}

%     \item Why its important to rank warnings (show the most relevant subset) or reduce false positive?
%     \begin{itemize}
%         \item The classic approach most automated code inspection tools use for prioritizing and filtering results is to classify the results based on severity levels. Such levels are (statically) associated with the type of defects detected; they are oblivious of the actual code that is being analyzed and of the location or frequency of a given defect.
%         \item Empirically, all tools that effectively find errors have false positive rates that can easily reach 30–100\%.
%         \item In general, there are roughly 40 warnings for every thousand lines of code [11]. This overload of warnings is a prime reason for developers to avoid using ASATs [12]. (\cite{analysis_sa_usage})
%         \item False reports can easily render tools useless by hiding real errors amidst the false, and by potentially causing the tool to be discarded as irrelevant.
%         \item Warnings are not always acted on by developers even if they reveal true defects. Reasons for defects being ignored include warnings implicating obsolete code, “trivial” defects with no impact on the user, and real defects requiring significant effort to fix with little perceived benefit.
%         \item The effort required to manually audit all alerts and repair all confirmed code flaws is often too much for a project’s budget and schedule.
%     \end{itemize}

%     \item How to improve?
%     \begin{itemize}
%         \item An understanding of how developers react on the alerts detected by SA tools can help improve the utility of these tools and determine future research directions.
%         \item The initial few error reports should be those most likely to be real errors so that the user can easily see if the rest of the errors are worth inspecting (otherwise users tend to discard the tool).
%         \item \textbf{Strategically prioritizing} alerts for examination.
%         \item Results indicate that using several ASATs has benefits over using just a single ASAT. (\cite{analysis_sa_usage})
%         \item Tracking warnings through a series of software versions reveals where potential defects are commonly introduced and addressed, and how long they persist; thus exposing interesting trends and patterns. This helps in determining which SA rules are important for a software system and helps select a minimum set of rules that must be enabled.
%     \end{itemize}

%     \item We use the term actionable alert (AA) to define a SA alert that the programmer would act on to resolve and unactionable alert (UA) to define a SA alert that the programmer would not act on to resolve.
    
%     \item Only show \textbf{actionable alerts}.
%     An unactionable alert may be one of the following: 1) a trivial concern to fix, 2) less likely to manifest in runtime environment; or 3) incorrectly identified due to the limitations of the tool.
    
    
%     % \item It is generally accepted that the cost of repairing a defect is much lower when that defect is found early in the development cycle.
    
%     % \item The above problem of alarms and associated cost can be addressed either by improving precision of the analysis or by postprocessing the alarms effectively after they are generated.
    
%     % \item Note that the above described handling of alarms does not consider reducing the number of alarms \textbf{by making underlined static analysis more precise}. That is, it excludes the option of improving precision of analyses, like value analysis and pointer analysis, implemented in the static analysis tools.
    
%     % \item However, even when tools focus on uncovering the same defect type, the variance in defects found is still very large [6],[13]–[15]. These results indicate that using several ASATs has benefits over using just a single ASAT. (\cite{analysis_sa_usage})
    
%     \item ranking schemes do not reduce alarm-investigation burdens - alleviate the false alarm problem by showing alarms that are most likely to be real errors over those that are least likely. However, the number of alarms to investigate is not reduced with ranking.
    
% \end{itemize}

