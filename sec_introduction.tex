% !TEX root = main.tex

\section{Introduction}\label{sec:introduction}
% This is a general introduction to what the thesis is all about -- it is not just a description of the contents of each section. Briefly summarize the question (you will be stating the question in detail later), some of the reasons why it is a worthwhile question, and perhaps give an overview of your main results. This is a birds-eye view of the answers to the main questions answered in the thesis (see above). 

% +Positive aspects of SA and usages (google, fb etc...), their need (huge amounts of code), etc...\\

The amount of code that is being produced is increasing with time. That brings multiple challenges in the software engineering landscape, one of which is assuring adequate code quality. In this context, automatic approaches such as static analysis can be really useful and time saving in detecting and preventing potential bugs. Its usefulness has been acknowledged also by big tech companies, such as Google with its Tricorder architecture (\cite{sa_google}) or Facebook with its Infer static analyzer.
Another reason to adopt SA techniques, is that the cost of repairing a defect is much lower when that defect is found early in the development cycle (generally accepted principle).

Even though it is a promising technique, in practice static SA results are far from perfect. Since the software under analysis is not executed, static analysis tools must speculate on what the actual program behavior will be, thus mistakes are inevitable. Heuristics or approximations are commonly used to determine properties for a given code construct, which may lead to inaccurate assumptions. A tool can also deliberately introduce approximations for scalability or speed, potentially resulting in an outbreak of errors.

Given the ever increasing amount of code and the tendency of SA tools to over-estimate possible faulty program behaviours, there is a need to improve the output of these tools. The above problem can be addressed either by increasing the precision of the analysis (and thus decrease the efficiency) or by post-processing the alarms effectively after they are generated, according to specific criteria.
The classic approach most tools use for prioritizing and filtering results is to statically classify the results based on severity levels. They are oblivious of the actual code that is being analyzed and of the location or frequency of a given defect. Furthermore, it has been shown that if developers lose trust in the beginning, they then tend to ignore the output of these tools altogether (\textbf{TO DO: CITE}).

Different approaches have been proposed to improve the output of SA tools. Optimally, the initial error reports should be those most likely to be real errors. Alerts can be strategically prioritized for examination, by tracking warnings through a series of software versions, revealing which SA rules are more important and which parts of the software are more problematic. 
% Furthermore, alerts can be grouped/clustered in basis of their similarity, so that users can check only a few and then can easily determine if the rest of the errors are also worth inspecting.\\
An understanding of how developers react on these alerts can help improve the utility of these tools. Alerts can be divided into two categories: actionable alerts (AA) to define a SA alert that the programmer would act on to resolve and unactionable alerts (UA) to define a SA alert that the programmer would not act on to resolve (\textbf{TO DO: CITE}). An unactionable alert may be: a trivial concern to fix, less likely to manifest at runtime, or incorrectly identified due to the limitations of the tool. We want thus to prioritize the AAs and hide the UAs.

Another approach is to prioritize alerts that in the past have pointed at bugs (\textbf{TO DO: CITE}). By tracking back the bugs up to a certain revision in the past, we can collect code lines that were changed during bug(\textbf{TO DO: CITE}) fixes. Subsequently, we can pinpoint which alerts warned about those specific parts of code and prioritize accordingly.

Given that for a relatively large project, the number of SA alerts can be prohibitive, the ultimate goal is to maximize the utility of time spent analyzing these alerts. Ranking schemes do not reduce alert investigation burdens if the aim is to check them all. Instead, they solve the problem by showing alerts that are most likely to be real/useful, so that developers can spend time by inspecting the most important ones.
% , but the number of alarms to investigate is not reduced with ranking.

Different approaches have been proposed in the literature but few have tried to do a comparison in terms of the utility of each method. Comparison is done among open source Java software and has the following problems... (\textbf{TO DO: FILL IN}). Also, these methods can be combined with the aim of achieving better results. It is important to do so because different techniques can be better suited to different types of alerts or can compensate for each others weaknesses.

% (depending also on the information that can be extracted from the source code).

% -what is this student's research question?
% -is it a good question? (has it been answered before? is it a useful question to work on?) 

Given an industrial code base, where SA tools have been abandoned because of the large amount of false positives, the goal of this thesis is to test the feasibility of successfully applying these ranking approaches even in a context with limited amount of data.

% The research questions can be formulated as follows:
% \begin{itemize}
%     \item R0: Can we apply SA ranking techniques in an industrial environment with limited amount of data (abandoned because of high false positives)?
%     \item R1: Given a highly imbalances dataset, what are the best performing dataset balancing techniques?
%     \item R2: Can we process the collected data to remove faulty examples (CLNI?) (project specific bugs?) (fix the data not the algorithms)
%     \item R3: Can we combine SA ranking techniques to achieve better results?
% \end{itemize}

% In this thesis, a comparison of different SA processing approaches is performed in an industrial codebase. Also, the utility of combining different methods is explored.

The rest of this thesis will be structured in the following sections... (\textbf{TODO})

% \textbf{TO DO: duality, actionable alert and bug predicition alerts}

% \textbf{TO DO: add class/method to each alert}

% \textbf{TO DO: Collect line from alert}

% \textbf{TO DO: Try clang automatic fixes}


% Arguments/information gotten from papers that can be used for the thesis introduction.
% \begin{itemize}
%     \item Why the false positives?
%     \begin{itemize}
%         \item Inevitable mistakes (SA has to make a trade-off with efficiency).
%         \item Static analyzers commonly use heuristics or approximations to determine properties for a given code construct, which frequently induces them to make inaccurate assumptions regarding the behavior of the program under evaluation.
%         \item A tool may deliberately introduce approximations for scalability or speed or to check richer properties than is generally possible, potentially resulting in an outbreak of errors. By ranking the results the invalid errors can be relegated below true errors.
%         \item Because the software under analysis is not executed, static analysis tools must speculate on what the actual program behavior will be. They often over-estimate possible program behaviors, leading to spurious warnings (“false positives”) that do not correspond to true defects.
%     \end{itemize}

%     \item Why its important to rank warnings (show the most relevant subset) or reduce false positive?
%     \begin{itemize}
%         \item The classic approach most automated code inspection tools use for prioritizing and filtering results is to classify the results based on severity levels. Such levels are (statically) associated with the type of defects detected; they are oblivious of the actual code that is being analyzed and of the location or frequency of a given defect.
%         \item Empirically, all tools that effectively find errors have false positive rates that can easily reach 30–100\%.
%         \item In general, there are roughly 40 warnings for every thousand lines of code [11]. This overload of warnings is a prime reason for developers to avoid using ASATs [12]. (\cite{analysis_sa_usage})
%         \item False reports can easily render tools useless by hiding real errors amidst the false, and by potentially causing the tool to be discarded as irrelevant.
%         \item Warnings are not always acted on by developers even if they reveal true defects. Reasons for defects being ignored include warnings implicating obsolete code, “trivial” defects with no impact on the user, and real defects requiring significant effort to fix with little perceived benefit.
%         \item The effort required to manually audit all alerts and repair all confirmed code flaws is often too much for a project’s budget and schedule.
%     \end{itemize}

%     \item How to improve?
%     \begin{itemize}
%         \item An understanding of how developers react on the alerts detected by SA tools can help improve the utility of these tools and determine future research directions.
%         \item The initial few error reports should be those most likely to be real errors so that the user can easily see if the rest of the errors are worth inspecting (otherwise users tend to discard the tool).
%         \item \textbf{Strategically prioritizing} alerts for examination.
%         \item Results indicate that using several ASATs has benefits over using just a single ASAT. (\cite{analysis_sa_usage})
%         \item Tracking warnings through a series of software versions reveals where potential defects are commonly introduced and addressed, and how long they persist; thus exposing interesting trends and patterns. This helps in determining which SA rules are important for a software system and helps select a minimum set of rules that must be enabled.
%     \end{itemize}

%     \item We use the term actionable alert (AA) to define a SA alert that the programmer would act on to resolve and unactionable alert (UA) to define a SA alert that the programmer would not act on to resolve.
    
%     \item Only show \textbf{actionable alerts}.
%     An unactionable alert may be one of the following: 1) a trivial concern to fix, 2) less likely to manifest in runtime environment; or 3) incorrectly identified due to the limitations of the tool.
    
    
%     % \item It is generally accepted that the cost of repairing a defect is much lower when that defect is found early in the development cycle.
    
%     % \item The above problem of alarms and associated cost can be addressed either by improving precision of the analysis or by postprocessing the alarms effectively after they are generated.
    
%     % \item Note that the above described handling of alarms does not consider reducing the number of alarms \textbf{by making underlined static analysis more precise}. That is, it excludes the option of improving precision of analyses, like value analysis and pointer analysis, implemented in the static analysis tools.
    
%     % \item However, even when tools focus on uncovering the same defect type, the variance in defects found is still very large [6],[13]–[15]. These results indicate that using several ASATs has benefits over using just a single ASAT. (\cite{analysis_sa_usage})
    
%     \item ranking schemes do not reduce alarm-investigation burdens - alleviate the false alarm problem by showing alarms that are most likely to be real errors over those that are least likely. However, the number of alarms to investigate is not reduced with ranking.
    
% \end{itemize}

